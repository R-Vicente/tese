{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ISCA-k v2 - Benchmark de Avaliação\n",
    "\n",
    "Este notebook é o **único ponto de avaliação** do método ISCA-k.\n",
    "\n",
    "Cada vez que modificamos o método (adicionamos pesos, PDS, k adaptativo, etc.), corremos este notebook para medir o impacto.\n",
    "\n",
    "## Estrutura\n",
    "1. **Funções de Avaliação** - Métricas (RMSE, R², etc.)\n",
    "2. **Funções de Missingness** - Introduzir MCAR, MAR, MNAR\n",
    "3. **Datasets** - Carregar dados de teste\n",
    "4. **Métodos de Imputação** - ISCA-k e baselines (KNN, MICE)\n",
    "5. **Benchmark** - Executar e comparar\n",
    "6. **Resultados** - Visualizar e analisar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "from typing import Tuple, Dict, List, Union, Optional\n",
    "\n",
    "# sklearn\n",
    "from sklearn.datasets import load_iris, load_diabetes, load_wine\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Funções de Avaliação (Métricas)\n",
    "\n",
    "Métricas para avaliar qualidade da imputação:\n",
    "- **RMSE**: Root Mean Squared Error\n",
    "- **NRMSE**: RMSE normalizado pelo range\n",
    "- **MAE**: Mean Absolute Error\n",
    "- **R²**: Coeficiente de determinação\n",
    "- **Accuracy**: Para variáveis categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Root Mean Squared Error.\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "\n",
    "def nrmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Normalized RMSE (pelo range).\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    value_range = float(np.max(y_true) - np.min(y_true))\n",
    "    if value_range == 0:\n",
    "        return np.nan\n",
    "    return rmse(y_true, y_pred) / value_range\n",
    "\n",
    "\n",
    "def mae(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Mean Absolute Error.\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "\n",
    "def r2_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Coeficiente de determinação R².\"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=np.float64)\n",
    "    y_pred = np.asarray(y_pred, dtype=np.float64)\n",
    "    \n",
    "    if len(y_true) < 2:\n",
    "        return np.nan\n",
    "    \n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    \n",
    "    if ss_tot == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return float(1 - ss_res / ss_tot)\n",
    "\n",
    "\n",
    "def accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Accuracy para variáveis categóricas.\"\"\"\n",
    "    matches = sum(1 for t, p in zip(y_true, y_pred) if t == p)\n",
    "    return float(matches / len(y_true))\n",
    "\n",
    "\n",
    "def compute_metrics(original: np.ndarray, imputed: np.ndarray, \n",
    "                    mask: np.ndarray, is_categorical: bool = False) -> Dict:\n",
    "    \"\"\"Calcula métricas para uma coluna imputada.\"\"\"\n",
    "    y_true = original[mask]\n",
    "    y_pred = imputed[mask]\n",
    "    \n",
    "    if is_categorical:\n",
    "        return {'accuracy': accuracy(y_true, y_pred), 'n': len(y_true)}\n",
    "    \n",
    "    y_true = y_true.astype(np.float64)\n",
    "    y_pred = y_pred.astype(np.float64)\n",
    "    \n",
    "    return {\n",
    "        'rmse': rmse(y_true, y_pred),\n",
    "        'nrmse': nrmse(y_true, y_pred),\n",
    "        'mae': mae(y_true, y_pred),\n",
    "        'r2': r2_score(y_true, y_pred),\n",
    "        'n': len(y_true)\n",
    "    }\n",
    "\n",
    "\n",
    "# Teste rápido\n",
    "y_true = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "y_pred = np.array([1.1, 2.0, 3.1, 4.0, 5.0])\n",
    "print(f\"RMSE teste: {rmse(y_true, y_pred):.4f} (esperado: ~0.063)\")\n",
    "print(f\"R² teste: {r2_score(y_true, y_pred):.4f} (esperado: ~0.99)\")\n",
    "print(\"Métricas OK ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missingness-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Funções de Missingness\n",
    "\n",
    "Introduzir valores em falta com diferentes padrões:\n",
    "- **MCAR**: Missing Completely At Random\n",
    "- **MAR**: Missing At Random (depende de outras variáveis)\n",
    "- **MNAR**: Missing Not At Random (depende do próprio valor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missingness-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_mcar(data: np.ndarray, missing_rate: float, \n",
    "                   random_state: int = 42) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Introduz missings MCAR (Missing Completely At Random).\n",
    "    \n",
    "    Returns:\n",
    "        (data_missing, mask) - mask é True onde valor foi removido\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    data = np.array(data, dtype=np.float64).copy()\n",
    "    n_rows, n_cols = data.shape\n",
    "    \n",
    "    # Gerar máscara\n",
    "    mask = rng.random((n_rows, n_cols)) < missing_rate\n",
    "    \n",
    "    # Garantir pelo menos 1 valor por linha\n",
    "    for i in range(n_rows):\n",
    "        if mask[i].all():\n",
    "            mask[i, rng.randint(n_cols)] = False\n",
    "    \n",
    "    # Garantir pelo menos 1 valor por coluna\n",
    "    for j in range(n_cols):\n",
    "        if mask[:, j].all():\n",
    "            mask[rng.randint(n_rows), j] = False\n",
    "    \n",
    "    data[mask] = np.nan\n",
    "    return data, mask\n",
    "\n",
    "\n",
    "def introduce_mar(data: np.ndarray, missing_rate: float,\n",
    "                  random_state: int = 42, driver_col: int = 0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Introduz missings MAR (Missing At Random).\n",
    "    Probabilidade depende da coluna driver.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    data = np.array(data, dtype=np.float64).copy()\n",
    "    n_rows, n_cols = data.shape\n",
    "    \n",
    "    driver = data[:, driver_col]\n",
    "    median = np.nanmedian(driver)\n",
    "    \n",
    "    prob_high = min(missing_rate * 1.5, 0.95)\n",
    "    prob_low = max(missing_rate * 0.5, 0.0)\n",
    "    \n",
    "    mask = np.zeros((n_rows, n_cols), dtype=bool)\n",
    "    \n",
    "    for i in range(n_rows):\n",
    "        prob = prob_high if driver[i] > median else prob_low\n",
    "        for j in range(n_cols):\n",
    "            if j != driver_col and rng.random() < prob:\n",
    "                mask[i, j] = True\n",
    "    \n",
    "    # Garantias\n",
    "    for i in range(n_rows):\n",
    "        if mask[i].all():\n",
    "            mask[i, rng.randint(n_cols)] = False\n",
    "    for j in range(n_cols):\n",
    "        if mask[:, j].all():\n",
    "            mask[rng.randint(n_rows), j] = False\n",
    "    \n",
    "    data[mask] = np.nan\n",
    "    return data, mask\n",
    "\n",
    "\n",
    "def introduce_mnar(data: np.ndarray, missing_rate: float,\n",
    "                   random_state: int = 42) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Introduz missings MNAR (Missing Not At Random).\n",
    "    Probabilidade depende do próprio valor.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    data = np.array(data, dtype=np.float64).copy()\n",
    "    n_rows, n_cols = data.shape\n",
    "    \n",
    "    prob_high = min(missing_rate * 1.5, 0.95)\n",
    "    prob_low = max(missing_rate * 0.5, 0.0)\n",
    "    \n",
    "    mask = np.zeros((n_rows, n_cols), dtype=bool)\n",
    "    \n",
    "    for j in range(n_cols):\n",
    "        col = data[:, j]\n",
    "        median = np.nanmedian(col)\n",
    "        for i in range(n_rows):\n",
    "            prob = prob_high if col[i] > median else prob_low\n",
    "            if rng.random() < prob:\n",
    "                mask[i, j] = True\n",
    "    \n",
    "    # Garantias\n",
    "    for i in range(n_rows):\n",
    "        if mask[i].all():\n",
    "            mask[i, rng.randint(n_cols)] = False\n",
    "    for j in range(n_cols):\n",
    "        if mask[:, j].all():\n",
    "            mask[rng.randint(n_rows), j] = False\n",
    "    \n",
    "    data[mask] = np.nan\n",
    "    return data, mask\n",
    "\n",
    "\n",
    "# Teste rápido\n",
    "test_data = np.random.randn(100, 5)\n",
    "data_missing, mask = introduce_mcar(test_data, 0.2)\n",
    "actual_rate = mask.sum() / mask.size\n",
    "print(f\"Taxa de missings: {actual_rate:.2%} (alvo: 20%)\")\n",
    "print(f\"Linhas 100% vazias: {(mask.all(axis=1)).sum()} (esperado: 0)\")\n",
    "print(f\"Colunas 100% vazias: {(mask.all(axis=0)).sum()} (esperado: 0)\")\n",
    "print(\"Missingness OK ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "datasets-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Datasets\n",
    "\n",
    "Datasets de teste:\n",
    "- **Iris**: 150 × 4 (pequeno, baixa dimensionalidade)\n",
    "- **Wine**: 178 × 13 (médio)\n",
    "- **Diabetes**: 442 × 10 (JÁ NORMALIZADO!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets():\n",
    "    \"\"\"Carrega todos os datasets de teste.\"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # Iris\n",
    "    iris = load_iris()\n",
    "    datasets['iris'] = {\n",
    "        'data': iris.data,\n",
    "        'names': iris.feature_names,\n",
    "        'is_normalized': False,\n",
    "        'description': 'Iris (150×4)'\n",
    "    }\n",
    "    \n",
    "    # Wine\n",
    "    wine = load_wine()\n",
    "    datasets['wine'] = {\n",
    "        'data': wine.data,\n",
    "        'names': wine.feature_names,\n",
    "        'is_normalized': False,\n",
    "        'description': 'Wine (178×13)'\n",
    "    }\n",
    "    \n",
    "    # Diabetes (ATENÇÃO: já normalizado!)\n",
    "    diabetes = load_diabetes()\n",
    "    datasets['diabetes'] = {\n",
    "        'data': diabetes.data,\n",
    "        'names': diabetes.feature_names,\n",
    "        'is_normalized': True,  # IMPORTANTE!\n",
    "        'description': 'Diabetes (442×10, JÁ NORMALIZADO)'\n",
    "    }\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Carregar\n",
    "DATASETS = load_datasets()\n",
    "\n",
    "print(\"Datasets carregados:\")\n",
    "for name, info in DATASETS.items():\n",
    "    data = info['data']\n",
    "    print(f\"  {name}: {data.shape[0]} amostras × {data.shape[1]} features\")\n",
    "    if info['is_normalized']:\n",
    "        print(f\"    ⚠️  JÁ NORMALIZADO (mean≈{data.mean():.3f}, std≈{data.std():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imputers-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Métodos de Imputação\n",
    "\n",
    "- **KNN (sklearn)**: Baseline\n",
    "- **MICE (sklearn)**: Baseline iterativo\n",
    "- **ISCA-k**: Nosso método (a ser implementado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imputation-methods",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_knn(data_missing: np.ndarray, k: int = 5) -> np.ndarray:\n",
    "    \"\"\"Imputa usando KNN do sklearn.\"\"\"\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    return imputer.fit_transform(data_missing)\n",
    "\n",
    "\n",
    "def impute_mice(data_missing: np.ndarray, max_iter: int = 10) -> np.ndarray:\n",
    "    \"\"\"Imputa usando MICE (IterativeImputer) do sklearn.\"\"\"\n",
    "    imputer = IterativeImputer(max_iter=max_iter, random_state=42)\n",
    "    return imputer.fit_transform(data_missing)\n",
    "\n",
    "\n",
    "def impute_iscak(data_missing: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Imputa usando ISCA-k.\n",
    "    \n",
    "    VERSÃO ACTUAL: KNN simples (baseline)\n",
    "    TODO: Adicionar componentes progressivamente\n",
    "    \"\"\"\n",
    "    # Por agora, igual ao KNN básico\n",
    "    # Vamos substituir gradualmente\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    return imputer.fit_transform(data_missing)\n",
    "\n",
    "\n",
    "# Dicionário de métodos\n",
    "METHODS = {\n",
    "    'KNN': impute_knn,\n",
    "    'MICE': impute_mice,\n",
    "    'ISCA-k': impute_iscak\n",
    "}\n",
    "\n",
    "print(f\"Métodos disponíveis: {list(METHODS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Benchmark\n",
    "\n",
    "Executa o benchmark completo:\n",
    "- Múltiplos datasets\n",
    "- Múltiplas taxas de missing\n",
    "- Múltiplos padrões (MCAR, MAR)\n",
    "- Múltiplos métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(data_original: np.ndarray, \n",
    "                          method_fn, \n",
    "                          missing_rate: float,\n",
    "                          pattern: str = 'MCAR',\n",
    "                          random_state: int = 42) -> Dict:\n",
    "    \"\"\"\n",
    "    Executa uma experiência única.\n",
    "    \n",
    "    Returns:\n",
    "        Dict com métricas agregadas\n",
    "    \"\"\"\n",
    "    # Introduzir missings\n",
    "    if pattern == 'MCAR':\n",
    "        data_missing, mask = introduce_mcar(data_original, missing_rate, random_state)\n",
    "    elif pattern == 'MAR':\n",
    "        data_missing, mask = introduce_mar(data_original, missing_rate, random_state)\n",
    "    elif pattern == 'MNAR':\n",
    "        data_missing, mask = introduce_mnar(data_original, missing_rate, random_state)\n",
    "    else:\n",
    "        raise ValueError(f\"Padrão desconhecido: {pattern}\")\n",
    "    \n",
    "    # Imputar\n",
    "    start = time.time()\n",
    "    try:\n",
    "        data_imputed = method_fn(data_missing)\n",
    "        elapsed = time.time() - start\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "    \n",
    "    # Calcular métricas por coluna\n",
    "    all_metrics = []\n",
    "    for j in range(data_original.shape[1]):\n",
    "        col_mask = mask[:, j]\n",
    "        if col_mask.sum() > 0:\n",
    "            m = compute_metrics(data_original[:, j], data_imputed[:, j], col_mask)\n",
    "            all_metrics.append(m)\n",
    "    \n",
    "    # Agregar\n",
    "    if not all_metrics:\n",
    "        return {'error': 'Sem valores para avaliar'}\n",
    "    \n",
    "    result = {\n",
    "        'rmse': np.nanmean([m['rmse'] for m in all_metrics]),\n",
    "        'nrmse': np.nanmean([m['nrmse'] for m in all_metrics]),\n",
    "        'mae': np.nanmean([m['mae'] for m in all_metrics]),\n",
    "        'r2': np.nanmean([m['r2'] for m in all_metrics]),\n",
    "        'time': elapsed,\n",
    "        'actual_missing_rate': mask.sum() / mask.size\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def run_benchmark(datasets: Dict = None,\n",
    "                  methods: Dict = None,\n",
    "                  missing_rates: List[float] = [0.1, 0.2, 0.3, 0.4],\n",
    "                  patterns: List[str] = ['MCAR', 'MAR'],\n",
    "                  n_runs: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Executa benchmark completo.\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dict de datasets (usa DATASETS global se None)\n",
    "        methods: Dict de métodos (usa METHODS global se None)\n",
    "        missing_rates: Lista de taxas de missing\n",
    "        patterns: Lista de padrões\n",
    "        n_runs: Número de repetições (com seeds diferentes)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com resultados\n",
    "    \"\"\"\n",
    "    if datasets is None:\n",
    "        datasets = DATASETS\n",
    "    if methods is None:\n",
    "        methods = METHODS\n",
    "    \n",
    "    results = []\n",
    "    total = len(datasets) * len(methods) * len(missing_rates) * len(patterns) * n_runs\n",
    "    current = 0\n",
    "    \n",
    "    for ds_name, ds_info in datasets.items():\n",
    "        data = ds_info['data']\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            for rate in missing_rates:\n",
    "                for method_name, method_fn in methods.items():\n",
    "                    for run in range(n_runs):\n",
    "                        current += 1\n",
    "                        seed = 42 + run\n",
    "                        \n",
    "                        print(f\"\\r[{current}/{total}] {ds_name} | {pattern} | {rate:.0%} | {method_name} | run {run+1}\", end=\"\")\n",
    "                        \n",
    "                        metrics = run_single_experiment(\n",
    "                            data, method_fn, rate, pattern, seed\n",
    "                        )\n",
    "                        \n",
    "                        results.append({\n",
    "                            'dataset': ds_name,\n",
    "                            'pattern': pattern,\n",
    "                            'missing_rate': f\"{int(rate*100)}%\",\n",
    "                            'method': method_name,\n",
    "                            'run': run,\n",
    "                            **metrics\n",
    "                        })\n",
    "    \n",
    "    print(\"\\nConcluído!\")\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Executar Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração\n",
    "MISSING_RATES = [0.1, 0.2, 0.3, 0.4]\n",
    "PATTERNS = ['MCAR', 'MAR']  # Ignoramos MNAR por agora\n",
    "N_RUNS = 3\n",
    "\n",
    "# Executar\n",
    "results_df = run_benchmark(\n",
    "    missing_rates=MISSING_RATES,\n",
    "    patterns=PATTERNS,\n",
    "    n_runs=N_RUNS\n",
    ")\n",
    "\n",
    "# Guardar\n",
    "results_df.to_csv('benchmark_results.csv', index=False)\n",
    "print(f\"\\nResultados guardados em benchmark_results.csv\")\n",
    "print(f\"Total de experiências: {len(results_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo por método\n",
    "print(\"=\" * 60)\n",
    "print(\"RESUMO POR MÉTODO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary = results_df.groupby('method').agg({\n",
    "    'r2': ['mean', 'std'],\n",
    "    'nrmse': ['mean', 'std'],\n",
    "    'time': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-by-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo por dataset\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESUMO POR DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for ds in results_df['dataset'].unique():\n",
    "    print(f\"\\n--- {ds.upper()} ---\")\n",
    "    ds_data = results_df[results_df['dataset'] == ds]\n",
    "    summary_ds = ds_data.groupby('method').agg({\n",
    "        'r2': 'mean',\n",
    "        'nrmse': 'mean'\n",
    "    }).round(4)\n",
    "    display(summary_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-by-rate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo por taxa de missing\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESUMO POR TAXA DE MISSING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for rate in results_df['missing_rate'].unique():\n",
    "    print(f\"\\n--- {rate} ---\")\n",
    "    rate_data = results_df[results_df['missing_rate'] == rate]\n",
    "    summary_rate = rate_data.groupby('method').agg({\n",
    "        'r2': 'mean',\n",
    "        'nrmse': 'mean'\n",
    "    }).round(4)\n",
    "    display(summary_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Comparação ISCA-k vs Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ISCA-k vs BASELINES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "iscak_results = results_df[results_df['method'] == 'ISCA-k']['r2'].mean()\n",
    "knn_results = results_df[results_df['method'] == 'KNN']['r2'].mean()\n",
    "mice_results = results_df[results_df['method'] == 'MICE']['r2'].mean()\n",
    "\n",
    "print(f\"\\nR² médio:\")\n",
    "print(f\"  ISCA-k: {iscak_results:.4f}\")\n",
    "print(f\"  KNN:    {knn_results:.4f}\")\n",
    "print(f\"  MICE:   {mice_results:.4f}\")\n",
    "\n",
    "print(f\"\\nDiferença ISCA-k vs KNN:  {iscak_results - knn_results:+.4f}\")\n",
    "print(f\"Diferença ISCA-k vs MICE: {iscak_results - mice_results:+.4f}\")\n",
    "\n",
    "if iscak_results >= knn_results:\n",
    "    print(\"\\n✅ ISCA-k ≥ KNN\")\n",
    "else:\n",
    "    print(\"\\n❌ ISCA-k < KNN (precisa melhorar)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
