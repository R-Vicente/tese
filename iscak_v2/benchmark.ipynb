{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISCA-k v2 - Notebook de Benchmark\n",
    "\n",
    "Este notebook e o **ponto central de avaliacao** do metodo ISCA-k.\n",
    "\n",
    "## Objectivo\n",
    "Testar sistematicamente o efeito de cada modificacao no metodo:\n",
    "1. **Fase 0**: Diagnostico (tipos, missingness, estrutura)\n",
    "2. **Fase 1**: Efeito da MI (Mutual Information) nos pesos\n",
    "3. **Fase 2**: Efeito da PDS (Partial Distance Strategy)\n",
    "4. **Fase 3**: Efeito do k adaptativo\n",
    "5. etc.\n",
    "\n",
    "## Estrutura do Notebook\n",
    "1. **Configuracao** - Imports e seed\n",
    "2. **Funcoes de Missingness** - MCAR, MAR, MNAR (documentadas)\n",
    "3. **Funcoes de Metricas** - R2, NRMSE, Accuracy (documentadas)\n",
    "4. **Datasets** - Carregar dados de teste\n",
    "5. **Metodos de Imputacao** - ISCA-k e baselines\n",
    "6. **Benchmark** - Execucao sistematica\n",
    "7. **Resultados** - Analise e visualizacao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuracao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.datasets import load_iris, load_diabetes, load_wine\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Seed para reproducibilidade\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Configuracao OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Funcoes de Insercao de Missings\n",
    "\n",
    "### Padroes de Missingness\n",
    "\n",
    "| Padrao | Descricao | P(missing) |\n",
    "|--------|-----------|------------|\n",
    "| **MCAR** | Missing Completely At Random | Constante, independente de X |\n",
    "| **MAR** | Missing At Random | Depende de variaveis OBSERVADAS |\n",
    "| **MNAR** | Missing Not At Random | Depende do PROPRIO valor em falta |\n",
    "\n",
    "### Limitacoes da Implementacao\n",
    "- **MAR**: Usa apenas 1 coluna driver (cenarios reais podem ter multiplas dependencias)\n",
    "- **MNAR**: Valores > mediana tem sempre mais missings (cenario real pode ser inverso)\n",
    "- Missings em diferentes colunas sao gerados independentemente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_mcar(data: pd.DataFrame, missing_rate: float = 0.2, \n",
    "                   random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Introduz missings MCAR (Missing Completely At Random).\n",
    "    \n",
    "    Algoritmo:\n",
    "    1. Gera mascara aleatoria uniforme com P(missing) = missing_rate\n",
    "    2. Garante que nenhuma linha fica 100% vazia\n",
    "    3. Garante que nenhuma coluna fica 100% vazia\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame original (sem missings)\n",
    "        missing_rate: Taxa de missings desejada (0 a 1)\n",
    "        random_state: Seed para reproducibilidade\n",
    "    \n",
    "    Returns:\n",
    "        (data_missing, missing_mask): DataFrame com missings e mascara booleana\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    data_missing = data.copy()\n",
    "    n_rows, n_cols = data.shape\n",
    "    \n",
    "    # Criar mascara aleatoria\n",
    "    mask = np.random.random((n_rows, n_cols)) < missing_rate\n",
    "    \n",
    "    # Constraint: pelo menos 1 valor por linha\n",
    "    for i in range(n_rows):\n",
    "        if mask[i].all():\n",
    "            keep_idx = np.random.randint(n_cols)\n",
    "            mask[i, keep_idx] = False\n",
    "    \n",
    "    # Constraint: pelo menos 1 valor por coluna\n",
    "    for j in range(n_cols):\n",
    "        if mask[:, j].all():\n",
    "            keep_idx = np.random.randint(n_rows)\n",
    "            mask[keep_idx, j] = False\n",
    "    \n",
    "    # Aplicar mascara\n",
    "    missing_mask = pd.DataFrame(mask, index=data.index, columns=data.columns)\n",
    "    for col in data.columns:\n",
    "        data_missing.loc[missing_mask[col], col] = np.nan\n",
    "    \n",
    "    return data_missing, missing_mask\n",
    "\n",
    "\n",
    "def introduce_mar(data: pd.DataFrame, missing_rate: float = 0.2,\n",
    "                  random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Introduz missings MAR (Missing At Random).\n",
    "    \n",
    "    Algoritmo:\n",
    "    1. Selecciona primeira coluna numerica como driver\n",
    "    2. Se driver > mediana: P(missing) = rate * 1.5\n",
    "    3. Se driver <= mediana: P(missing) = rate * 0.5\n",
    "    4. Driver nunca tem missings (para manter dependencia observavel)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    data_missing = data.copy()\n",
    "    n_rows, n_cols = data.shape\n",
    "    \n",
    "    # Encontrar coluna driver (primeira numerica)\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) == 0:\n",
    "        return introduce_mcar(data, missing_rate, random_state)\n",
    "    \n",
    "    driver_col = numeric_cols[0]\n",
    "    driver_values = data[driver_col].values\n",
    "    driver_median = np.nanmedian(driver_values)\n",
    "    \n",
    "    prob_high = min(missing_rate * 1.5, 0.95)\n",
    "    prob_low = max(missing_rate * 0.5, 0.0)\n",
    "    \n",
    "    mask = np.zeros((n_rows, n_cols), dtype=bool)\n",
    "    \n",
    "    for i in range(n_rows):\n",
    "        prob = prob_high if driver_values[i] > driver_median else prob_low\n",
    "        for j, col in enumerate(data.columns):\n",
    "            if col == driver_col:\n",
    "                continue\n",
    "            if np.random.random() < prob:\n",
    "                mask[i, j] = True\n",
    "    \n",
    "    # Constraints\n",
    "    for i in range(n_rows):\n",
    "        if mask[i].all():\n",
    "            keep_idx = np.random.randint(n_cols)\n",
    "            mask[i, keep_idx] = False\n",
    "    \n",
    "    for j in range(n_cols):\n",
    "        if mask[:, j].all():\n",
    "            keep_idx = np.random.randint(n_rows)\n",
    "            mask[keep_idx, j] = False\n",
    "    \n",
    "    missing_mask = pd.DataFrame(mask, index=data.index, columns=data.columns)\n",
    "    for col in data.columns:\n",
    "        data_missing.loc[missing_mask[col], col] = np.nan\n",
    "    \n",
    "    return data_missing, missing_mask\n",
    "\n",
    "\n",
    "def introduce_mnar(data: pd.DataFrame, missing_rate: float = 0.2,\n",
    "                   random_state: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Introduz missings MNAR (Missing Not At Random).\n",
    "    \n",
    "    Algoritmo:\n",
    "    1. Para cada coluna independentemente:\n",
    "    2. Se valor > mediana: P(missing) = rate * 1.5\n",
    "    3. Se valor <= mediana: P(missing) = rate * 0.5\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    data_missing = data.copy()\n",
    "    n_rows, n_cols = data.shape\n",
    "    \n",
    "    prob_high = min(missing_rate * 1.5, 0.95)\n",
    "    prob_low = max(missing_rate * 0.5, 0.0)\n",
    "    \n",
    "    mask = np.zeros((n_rows, n_cols), dtype=bool)\n",
    "    \n",
    "    for j, col in enumerate(data.columns):\n",
    "        col_data = data[col]\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(col_data):\n",
    "            col_median = col_data.median()\n",
    "            for i in range(n_rows):\n",
    "                prob = prob_high if col_data.iloc[i] > col_median else prob_low\n",
    "                if np.random.random() < prob:\n",
    "                    mask[i, j] = True\n",
    "        else:\n",
    "            mode_val = col_data.mode().iloc[0] if len(col_data.mode()) > 0 else None\n",
    "            for i in range(n_rows):\n",
    "                prob = prob_high if col_data.iloc[i] == mode_val else prob_low\n",
    "                if np.random.random() < prob:\n",
    "                    mask[i, j] = True\n",
    "    \n",
    "    # Constraints\n",
    "    for i in range(n_rows):\n",
    "        if mask[i].all():\n",
    "            keep_idx = np.random.randint(n_cols)\n",
    "            mask[i, keep_idx] = False\n",
    "    \n",
    "    for j in range(n_cols):\n",
    "        if mask[:, j].all():\n",
    "            keep_idx = np.random.randint(n_rows)\n",
    "            mask[keep_idx, j] = False\n",
    "    \n",
    "    missing_mask = pd.DataFrame(mask, index=data.index, columns=data.columns)\n",
    "    for col in data.columns:\n",
    "        data_missing.loc[missing_mask[col], col] = np.nan\n",
    "    \n",
    "    return data_missing, missing_mask\n",
    "\n",
    "\n",
    "MISSINGNESS_FUNCTIONS = {\n",
    "    'MCAR': introduce_mcar,\n",
    "    'MAR': introduce_mar,\n",
    "    'MNAR': introduce_mnar\n",
    "}\n",
    "\n",
    "print(\"Funcoes de missingness OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Funcoes de Metricas\n",
    "\n",
    "### Metricas para Variaveis Numericas\n",
    "\n",
    "| Metrica | Formula | Range | Interpretacao |\n",
    "|---------|---------|-------|---------------|\n",
    "| **R2** | 1 - SS_res/SS_tot | (-inf, 1] | 1=perfeito, 0=media, <0=pior que media |\n",
    "| **Pearson** | cov(y,y')/(s_y*s_y') | [-1, 1] | 1=correlacao perfeita positiva |\n",
    "| **NRMSE** | RMSE/(max-min) | [0, inf) | 0=perfeito, normalizado pelo range |\n",
    "\n",
    "### Metricas para Variaveis Categoricas\n",
    "\n",
    "| Metrica | Formula | Range | Interpretacao |\n",
    "|---------|---------|-------|---------------|\n",
    "| **Accuracy** | n_correct/n_total | [0, 1] | 1=perfeito |\n",
    "\n",
    "### IMPORTANTE\n",
    "As metricas sao calculadas **apenas nos valores que foram imputados** (usando a missing_mask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nrmse(true_values: np.ndarray, imputed_values: np.ndarray) -> float:\n",
    "    \"\"\"Calcula NRMSE (Normalized Root Mean Squared Error).\"\"\"\n",
    "    valid = ~np.isnan(true_values) & ~np.isnan(imputed_values)\n",
    "    if valid.sum() == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    true_subset = true_values[valid]\n",
    "    imputed_subset = imputed_values[valid]\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((true_subset - imputed_subset) ** 2))\n",
    "    value_range = true_subset.max() - true_subset.min()\n",
    "    \n",
    "    if value_range == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return rmse / value_range\n",
    "\n",
    "\n",
    "def calculate_metrics_per_column(original_data: pd.DataFrame, \n",
    "                                  imputed_data: pd.DataFrame,\n",
    "                                  missing_mask: pd.DataFrame,\n",
    "                                  col_types: Dict[str, str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calcula metricas por coluna, avaliando APENAS os valores imputados.\n",
    "    \n",
    "    IMPORTANTE: Usa missing_mask para filtrar apenas os valores que\n",
    "    foram introduzidos como missing e depois imputados.\n",
    "    \"\"\"\n",
    "    r2_scores = []\n",
    "    pearson_scores = []\n",
    "    nrmse_scores = []\n",
    "    accuracy_scores = []\n",
    "    \n",
    "    for col in original_data.columns:\n",
    "        col_mask = missing_mask[col].values\n",
    "        if col_mask.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        true_values = original_data.loc[col_mask, col].values\n",
    "        imputed_values = imputed_data.loc[col_mask, col].values\n",
    "        \n",
    "        is_categorical = col_types.get(col, 'numeric') == 'categorical'\n",
    "        \n",
    "        if is_categorical:\n",
    "            valid = np.array([\n",
    "                v is not None and str(v).lower() != 'nan' and pd.notna(v)\n",
    "                for v in imputed_values\n",
    "            ])\n",
    "            \n",
    "            if valid.sum() < 1:\n",
    "                continue\n",
    "            \n",
    "            true_str = [str(x) for x in true_values[valid]]\n",
    "            imputed_str = [str(x) for x in imputed_values[valid]]\n",
    "            \n",
    "            try:\n",
    "                acc = accuracy_score(true_str, imputed_str)\n",
    "                accuracy_scores.append(acc)\n",
    "            except Exception:\n",
    "                pass\n",
    "        else:\n",
    "            try:\n",
    "                true_float = true_values.astype(float)\n",
    "                imputed_float = imputed_values.astype(float)\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "            \n",
    "            valid = ~np.isnan(imputed_float) & ~np.isnan(true_float)\n",
    "            if valid.sum() < 2:\n",
    "                continue\n",
    "            \n",
    "            true_subset = true_float[valid]\n",
    "            imputed_subset = imputed_float[valid]\n",
    "            \n",
    "            if np.std(true_subset) > 0:\n",
    "                try:\n",
    "                    r2 = r2_score(true_subset, imputed_subset)\n",
    "                    r2_scores.append(r2)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            if np.std(true_subset) > 0 and np.std(imputed_subset) > 0:\n",
    "                try:\n",
    "                    corr, _ = pearsonr(true_subset, imputed_subset)\n",
    "                    if np.isfinite(corr):\n",
    "                        pearson_scores.append(corr)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            \n",
    "            nrmse = calculate_nrmse(true_subset, imputed_subset)\n",
    "            if np.isfinite(nrmse):\n",
    "                nrmse_scores.append(nrmse)\n",
    "    \n",
    "    return {\n",
    "        'R2': np.mean(r2_scores) if r2_scores else np.nan,\n",
    "        'Pearson': np.mean(pearson_scores) if pearson_scores else np.nan,\n",
    "        'NRMSE': np.mean(nrmse_scores) if nrmse_scores else np.nan,\n",
    "        'Accuracy': np.mean(accuracy_scores) if accuracy_scores else np.nan\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Funcoes de metricas OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_benchmark_datasets() -> Dict:\n",
    "    datasets = {}\n",
    "    \n",
    "    iris = load_iris()\n",
    "    datasets['iris'] = {\n",
    "        'data': pd.DataFrame(iris.data, columns=iris.feature_names),\n",
    "        'col_types': {col: 'numeric' for col in iris.feature_names},\n",
    "        'description': 'Iris (150x4, numerico)'\n",
    "    }\n",
    "    \n",
    "    wine = load_wine()\n",
    "    datasets['wine'] = {\n",
    "        'data': pd.DataFrame(wine.data, columns=wine.feature_names),\n",
    "        'col_types': {col: 'numeric' for col in wine.feature_names},\n",
    "        'description': 'Wine (178x13, numerico)'\n",
    "    }\n",
    "    \n",
    "    diabetes = load_diabetes()\n",
    "    datasets['diabetes'] = {\n",
    "        'data': pd.DataFrame(diabetes.data, columns=diabetes.feature_names),\n",
    "        'col_types': {col: 'numeric' for col in diabetes.feature_names},\n",
    "        'description': 'Diabetes (442x10, numerico, JA NORMALIZADO)'\n",
    "    }\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "DATASETS = load_benchmark_datasets()\n",
    "\n",
    "print(\"Datasets carregados:\")\n",
    "for name, info in DATASETS.items():\n",
    "    shape = info['data'].shape\n",
    "    print(f\"  {name}: {shape[0]} amostras x {shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Metodos de Imputacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_knn(data_missing: pd.DataFrame, n_neighbors: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"Imputa usando KNN do sklearn.\"\"\"\n",
    "    data_encoded = data_missing.copy()\n",
    "    encoders = {}\n",
    "    \n",
    "    for col in data_encoded.columns:\n",
    "        if data_encoded[col].dtype == 'object' or data_encoded[col].dtype.name == 'category':\n",
    "            le = LabelEncoder()\n",
    "            non_null = data_encoded[col].dropna()\n",
    "            if len(non_null) > 0:\n",
    "                le.fit(non_null)\n",
    "                encoders[col] = le\n",
    "                mask = data_encoded[col].notna()\n",
    "                data_encoded.loc[mask, col] = le.transform(data_encoded.loc[mask, col])\n",
    "            data_encoded[col] = pd.to_numeric(data_encoded[col], errors='coerce')\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    imputed_array = imputer.fit_transform(data_encoded)\n",
    "    \n",
    "    result = pd.DataFrame(imputed_array, columns=data_missing.columns, index=data_missing.index)\n",
    "    \n",
    "    for col, le in encoders.items():\n",
    "        result[col] = result[col].round().astype(int)\n",
    "        result[col] = result[col].clip(0, len(le.classes_) - 1)\n",
    "        result[col] = le.inverse_transform(result[col])\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def impute_mice(data_missing: pd.DataFrame, max_iter: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Imputa usando MICE (IterativeImputer) do sklearn.\"\"\"\n",
    "    data_encoded = data_missing.copy()\n",
    "    encoders = {}\n",
    "    \n",
    "    for col in data_encoded.columns:\n",
    "        if data_encoded[col].dtype == 'object' or data_encoded[col].dtype.name == 'category':\n",
    "            le = LabelEncoder()\n",
    "            non_null = data_encoded[col].dropna()\n",
    "            if len(non_null) > 0:\n",
    "                le.fit(non_null)\n",
    "                encoders[col] = le\n",
    "                mask = data_encoded[col].notna()\n",
    "                data_encoded.loc[mask, col] = le.transform(data_encoded.loc[mask, col])\n",
    "            data_encoded[col] = pd.to_numeric(data_encoded[col], errors='coerce')\n",
    "    \n",
    "    imputer = IterativeImputer(max_iter=max_iter, random_state=42)\n",
    "    imputed_array = imputer.fit_transform(data_encoded)\n",
    "    \n",
    "    result = pd.DataFrame(imputed_array, columns=data_missing.columns, index=data_missing.index)\n",
    "    \n",
    "    for col, le in encoders.items():\n",
    "        result[col] = result[col].round().astype(int)\n",
    "        result[col] = result[col].clip(0, len(le.classes_) - 1)\n",
    "        result[col] = le.inverse_transform(result[col])\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def impute_iscak(data_missing: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Imputa usando ISCA-k.\n",
    "    \n",
    "    VERSAO ACTUAL: KNN simples (baseline)\n",
    "    \n",
    "    TODO: Modificar progressivamente para testar:\n",
    "    - [ ] Pesos MI\n",
    "    - [ ] PDS\n",
    "    - [ ] k adaptativo\n",
    "    \"\"\"\n",
    "    return impute_knn(data_missing, n_neighbors=5)\n",
    "\n",
    "\n",
    "IMPUTATION_METHODS = {\n",
    "    'KNN': impute_knn,\n",
    "    'MICE': impute_mice,\n",
    "    'ISCA-k': impute_iscak\n",
    "}\n",
    "\n",
    "print(f\"Metodos de imputacao: {list(IMPUTATION_METHODS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(data_original, col_types, method_name, method_fn,\n",
    "                          missing_rate, pattern, random_state):\n",
    "    data_missing, missing_mask = MISSINGNESS_FUNCTIONS[pattern](\n",
    "        data_original, missing_rate, random_state\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    try:\n",
    "        data_imputed = method_fn(data_missing)\n",
    "        elapsed = time.time() - start\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}\n",
    "    \n",
    "    metrics = calculate_metrics_per_column(\n",
    "        data_original, data_imputed, missing_mask, col_types\n",
    "    )\n",
    "    \n",
    "    actual_rate = missing_mask.sum().sum() / missing_mask.size\n",
    "    \n",
    "    return {**metrics, 'Time_s': elapsed, 'Actual_Rate': actual_rate}\n",
    "\n",
    "\n",
    "def run_benchmark(datasets=None, methods=None,\n",
    "                  missing_rates=[0.1, 0.2, 0.3, 0.4],\n",
    "                  patterns=['MCAR', 'MAR'],\n",
    "                  n_runs=3, verbose=True):\n",
    "    if datasets is None:\n",
    "        datasets = DATASETS\n",
    "    if methods is None:\n",
    "        methods = IMPUTATION_METHODS\n",
    "    \n",
    "    results = []\n",
    "    total = len(datasets) * len(methods) * len(missing_rates) * len(patterns) * n_runs\n",
    "    current = 0\n",
    "    \n",
    "    for ds_name, ds_info in datasets.items():\n",
    "        data = ds_info['data']\n",
    "        col_types = ds_info['col_types']\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            for rate in missing_rates:\n",
    "                for method_name, method_fn in methods.items():\n",
    "                    for run in range(n_runs):\n",
    "                        current += 1\n",
    "                        seed = RANDOM_SEED + run\n",
    "                        \n",
    "                        if verbose:\n",
    "                            print(f\"\\r[{current}/{total}] {ds_name} | {pattern} | {rate:.0%} | {method_name}\", end=\"\")\n",
    "                        \n",
    "                        metrics = run_single_experiment(\n",
    "                            data, col_types, method_name, method_fn,\n",
    "                            rate, pattern, seed\n",
    "                        )\n",
    "                        \n",
    "                        results.append({\n",
    "                            'Dataset': ds_name,\n",
    "                            'Pattern': pattern,\n",
    "                            'Missing_Rate': f\"{int(rate*100)}%\",\n",
    "                            'Method': method_name,\n",
    "                            'Run': run,\n",
    "                            **metrics\n",
    "                        })\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\nConcluido!\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "print(\"Funcao de benchmark OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Executar Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISSING_RATES = [0.1, 0.2, 0.3, 0.4]\n",
    "PATTERNS = ['MCAR', 'MAR']\n",
    "N_RUNS = 3\n",
    "\n",
    "print(\"Configuracao:\")\n",
    "print(f\"  Taxas de missing: {MISSING_RATES}\")\n",
    "print(f\"  Padroes: {PATTERNS}\")\n",
    "print(f\"  Repeticoes: {N_RUNS}\")\n",
    "print()\n",
    "\n",
    "results_df = run_benchmark(\n",
    "    missing_rates=MISSING_RATES,\n",
    "    patterns=PATTERNS,\n",
    "    n_runs=N_RUNS\n",
    ")\n",
    "\n",
    "results_df.to_csv('benchmark_results.csv', index=False)\n",
    "print(f\"\\nResultados guardados em benchmark_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Analise de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RESUMO POR METODO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary = results_df.groupby('Method').agg({\n",
    "    'R2': ['mean', 'std'],\n",
    "    'NRMSE': ['mean', 'std'],\n",
    "    'Time_s': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESUMO POR DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for ds in results_df['Dataset'].unique():\n",
    "    print(f\"\\n--- {ds.upper()} ---\")\n",
    "    ds_data = results_df[results_df['Dataset'] == ds]\n",
    "    summary_ds = ds_data.groupby('Method').agg({\n",
    "        'R2': 'mean',\n",
    "        'NRMSE': 'mean'\n",
    "    }).round(4)\n",
    "    display(summary_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ISCA-k vs BASELINES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "iscak_r2 = results_df[results_df['Method'] == 'ISCA-k']['R2'].mean()\n",
    "knn_r2 = results_df[results_df['Method'] == 'KNN']['R2'].mean()\n",
    "mice_r2 = results_df[results_df['Method'] == 'MICE']['R2'].mean()\n",
    "\n",
    "print(f\"\\nR2 medio:\")\n",
    "print(f\"  ISCA-k: {iscak_r2:.4f}\")\n",
    "print(f\"  KNN:    {knn_r2:.4f}\")\n",
    "print(f\"  MICE:   {mice_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nDiferenca ISCA-k vs KNN:  {iscak_r2 - knn_r2:+.4f}\")\n",
    "print(f\"Diferenca ISCA-k vs MICE: {iscak_r2 - mice_r2:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Notas e Proximos Passos\n",
    "\n",
    "### Estado Actual\n",
    "- ISCA-k = KNN (baseline)\n",
    "- Metricas avaliadas apenas em valores imputados\n",
    "- Funcoes de missingness documentadas\n",
    "\n",
    "### Proximas Modificacoes a Testar\n",
    "1. **MI (Mutual Information)**: Modificar `impute_iscak` para usar pesos MI\n",
    "2. **PDS**: Adicionar Partial Distance Strategy\n",
    "3. **k adaptativo**: Variar k baseado em densidade/consistencia\n",
    "\n",
    "### Como Usar Este Notebook\n",
    "1. Modificar `impute_iscak()` com nova funcionalidade\n",
    "2. Correr celulas 7 e 8\n",
    "3. Analisar diferencas nos resultados\n",
    "4. Documentar conclusoes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
