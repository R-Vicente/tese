{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Benchmark de Métodos de Imputação\n",
    "\n",
    "Este notebook compara o desempenho do ISCA-k com outros métodos de imputação:\n",
    "- **ISCA-k**: Método proposto\n",
    "- **KNN Imputer**: sklearn\n",
    "- **MICE (IterativeImputer)**: sklearn\n",
    "- **MissForest**: missforest package\n",
    "\n",
    "## Métricas\n",
    "- **Numéricas**: R², Pearson, NRMSE\n",
    "- **Categóricas**: Accuracy\n",
    "- **Tempo**: segundos\n",
    "\n",
    "## Padrões de Missingness\n",
    "- **MCAR**: Missing Completely At Random\n",
    "- **MAR**: Missing At Random\n",
    "- **MNAR**: Missing Not At Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.datasets import load_iris, load_diabetes\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from imputers.iscak_imputer import ISCAkCore\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utils-header",
   "metadata": {},
   "source": [
    "## Funções Utilitárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missingness-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_mcar(data, missing_rate=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Introduz missings MCAR (Missing Completely At Random).\n",
    "    Garante que nenhuma linha ou coluna fica 100% vazia.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    data_missing = data.copy()\n",
    "    n_rows, n_cols = data.shape\n",
    "    \n",
    "    # Criar máscara de missings\n",
    "    mask = np.random.random((n_rows, n_cols)) < missing_rate\n",
    "    \n",
    "    # Garantir pelo menos 1 valor por linha\n",
    "    for i in range(n_rows):\n",
    "        if mask[i].all():\n",
    "            # Manter pelo menos um valor\n",
    "            keep_idx = np.random.randint(n_cols)\n",
    "            mask[i, keep_idx] = False\n",
    "    \n",
    "    # Garantir pelo menos 1 valor por coluna\n",
    "    for j in range(n_cols):\n",
    "        if mask[:, j].all():\n",
    "            # Manter pelo menos um valor\n",
    "            keep_idx = np.random.randint(n_rows)\n",
    "            mask[keep_idx, j] = False\n",
    "    \n",
    "    # Aplicar máscara\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            if mask[i, j]:\n",
    "                data_missing.iloc[i, j] = np.nan\n",
    "    \n",
    "    return data_missing\n",
    "\n",
    "\n",
    "def introduce_mar(data, missing_rate=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Introduz missings MAR (Missing At Random).\n",
    "    A probabilidade de missing depende de outras variáveis observadas.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    data_missing = data.copy()\n",
    "    n_rows, n_cols = data.shape\n",
    "    \n",
    "    # Escolher uma coluna como \"driver\" (a primeira numérica)\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        driver_col = numeric_cols[0]\n",
    "        driver_values = data[driver_col].values\n",
    "        driver_median = np.nanmedian(driver_values)\n",
    "        \n",
    "        # Probabilidade de missing é maior quando driver > mediana\n",
    "        prob_high = missing_rate * 1.5\n",
    "        prob_low = missing_rate * 0.5\n",
    "    else:\n",
    "        # Fallback para MCAR se não há colunas numéricas\n",
    "        return introduce_mcar(data, missing_rate, random_state)\n",
    "    \n",
    "    mask = np.zeros((n_rows, n_cols), dtype=bool)\n",
    "    \n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            if data.columns[j] == driver_col:\n",
    "                continue  # Não introduzir missing no driver\n",
    "            \n",
    "            if driver_values[i] > driver_median:\n",
    "                prob = prob_high\n",
    "            else:\n",
    "                prob = prob_low\n",
    "            \n",
    "            if np.random.random() < prob:\n",
    "                mask[i, j] = True\n",
    "    \n",
    "    # Garantir pelo menos 1 valor por linha e coluna\n",
    "    for i in range(n_rows):\n",
    "        if mask[i].all():\n",
    "            keep_idx = np.random.randint(n_cols)\n",
    "            mask[i, keep_idx] = False\n",
    "    \n",
    "    for j in range(n_cols):\n",
    "        if mask[:, j].all():\n",
    "            keep_idx = np.random.randint(n_rows)\n",
    "            mask[keep_idx, j] = False\n",
    "    \n",
    "    # Aplicar máscara\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            if mask[i, j]:\n",
    "                data_missing.iloc[i, j] = np.nan\n",
    "    \n",
    "    return data_missing\n",
    "\n",
    "\n",
    "def introduce_mnar(data, missing_rate=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Introduz missings MNAR (Missing Not At Random).\n",
    "    A probabilidade de missing depende do próprio valor.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    data_missing = data.copy()\n",
    "    n_rows, n_cols = data.shape\n",
    "    \n",
    "    mask = np.zeros((n_rows, n_cols), dtype=bool)\n",
    "    \n",
    "    for j in range(n_cols):\n",
    "        col_data = data.iloc[:, j]\n",
    "        \n",
    "        if pd.api.types.is_numeric_dtype(col_data):\n",
    "            # Para numéricas: valores altos têm maior prob de missing\n",
    "            col_median = col_data.median()\n",
    "            for i in range(n_rows):\n",
    "                if col_data.iloc[i] > col_median:\n",
    "                    prob = missing_rate * 1.5\n",
    "                else:\n",
    "                    prob = missing_rate * 0.5\n",
    "                \n",
    "                if np.random.random() < prob:\n",
    "                    mask[i, j] = True\n",
    "        else:\n",
    "            # Para categóricas: categoria mais frequente tem maior prob de missing\n",
    "            mode_val = col_data.mode().iloc[0] if len(col_data.mode()) > 0 else None\n",
    "            for i in range(n_rows):\n",
    "                if col_data.iloc[i] == mode_val:\n",
    "                    prob = missing_rate * 1.5\n",
    "                else:\n",
    "                    prob = missing_rate * 0.5\n",
    "                \n",
    "                if np.random.random() < prob:\n",
    "                    mask[i, j] = True\n",
    "    \n",
    "    # Garantir pelo menos 1 valor por linha e coluna\n",
    "    for i in range(n_rows):\n",
    "        if mask[i].all():\n",
    "            keep_idx = np.random.randint(n_cols)\n",
    "            mask[i, keep_idx] = False\n",
    "    \n",
    "    for j in range(n_cols):\n",
    "        if mask[:, j].all():\n",
    "            keep_idx = np.random.randint(n_rows)\n",
    "            mask[keep_idx, j] = False\n",
    "    \n",
    "    # Aplicar máscara\n",
    "    for i in range(n_rows):\n",
    "        for j in range(n_cols):\n",
    "            if mask[i, j]:\n",
    "                data_missing.iloc[i, j] = np.nan\n",
    "    \n",
    "    return data_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nrmse(true_values, imputed_values):\n",
    "    \"\"\"Calcula NRMSE (Normalized Root Mean Squared Error).\"\"\"\n",
    "    mask = ~np.isnan(true_values) & ~np.isnan(imputed_values)\n",
    "    if mask.sum() == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    true_subset = true_values[mask]\n",
    "    imputed_subset = imputed_values[mask]\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((true_subset - imputed_subset) ** 2))\n",
    "    value_range = true_subset.max() - true_subset.min()\n",
    "    \n",
    "    if value_range == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return rmse / value_range\n",
    "\n",
    "\n",
    "def calculate_metrics_per_column(original_data, imputed_data, missing_mask, col_types):\n",
    "    \"\"\"\n",
    "    Calcula métricas por coluna e retorna a média.\n",
    "    \n",
    "    Args:\n",
    "        original_data: DataFrame original (sem missings)\n",
    "        imputed_data: DataFrame imputado\n",
    "        missing_mask: Máscara booleana de onde havia missings\n",
    "        col_types: Dict com tipo de cada coluna ('numeric' ou 'categorical')\n",
    "    \n",
    "    Returns:\n",
    "        Dict com métricas médias\n",
    "    \"\"\"\n",
    "    r2_scores = []\n",
    "    pearson_scores = []\n",
    "    nrmse_scores = []\n",
    "    accuracy_scores = []\n",
    "    \n",
    "    for col in original_data.columns:\n",
    "        col_mask = missing_mask[col].values\n",
    "        if col_mask.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        true_values = original_data.loc[col_mask, col].values\n",
    "        imputed_values = imputed_data.loc[col_mask, col].values\n",
    "        \n",
    "        # Remover NaN residuais\n",
    "        valid_mask = ~np.isnan(imputed_values.astype(float))\n",
    "        if valid_mask.sum() < 2:\n",
    "            continue\n",
    "        \n",
    "        true_subset = true_values[valid_mask]\n",
    "        imputed_subset = imputed_values[valid_mask]\n",
    "        \n",
    "        if col_types.get(col, 'numeric') == 'numeric':\n",
    "            # Converter para float\n",
    "            true_float = true_subset.astype(float)\n",
    "            imputed_float = imputed_subset.astype(float)\n",
    "            \n",
    "            # R²\n",
    "            if len(true_float) >= 2 and np.std(true_float) > 0:\n",
    "                try:\n",
    "                    r2 = r2_score(true_float, imputed_float)\n",
    "                    r2_scores.append(r2)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Pearson\n",
    "            if len(true_float) >= 2 and np.std(true_float) > 0 and np.std(imputed_float) > 0:\n",
    "                try:\n",
    "                    corr, _ = pearsonr(true_float, imputed_float)\n",
    "                    if np.isfinite(corr):\n",
    "                        pearson_scores.append(corr)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # NRMSE\n",
    "            nrmse = calculate_nrmse(true_float, imputed_float)\n",
    "            if np.isfinite(nrmse):\n",
    "                nrmse_scores.append(nrmse)\n",
    "        else:\n",
    "            # Accuracy para categóricas\n",
    "            try:\n",
    "                # Converter para string para comparação\n",
    "                true_str = [str(x) for x in true_subset]\n",
    "                imputed_str = [str(x) for x in imputed_subset]\n",
    "                acc = accuracy_score(true_str, imputed_str)\n",
    "                accuracy_scores.append(acc)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    return {\n",
    "        'R2': np.mean(r2_scores) if r2_scores else np.nan,\n",
    "        'Pearson': np.mean(pearson_scores) if pearson_scores else np.nan,\n",
    "        'NRMSE': np.mean(nrmse_scores) if nrmse_scores else np.nan,\n",
    "        'Accuracy': np.mean(accuracy_scores) if accuracy_scores else np.nan\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imputation-functions",
   "metadata": {},
   "outputs": [],
   "source": "def impute_with_iscak(data_missing, verbose=False):\n    \"\"\"Imputa usando ISCA-k.\"\"\"\n    imputer = ISCAkCore(verbose=verbose, fast_mode=True)\n    start = time.time()\n    result = imputer.impute(data_missing, interactive=False)\n    elapsed = time.time() - start\n    return result, elapsed\n\n\ndef impute_with_knn(data_missing, n_neighbors=5):\n    \"\"\"Imputa usando KNN Imputer do sklearn.\"\"\"\n    # KNN só funciona com dados numéricos\n    data_encoded = data_missing.copy()\n    encoders = {}\n    \n    for col in data_encoded.columns:\n        if data_encoded[col].dtype == 'object' or data_encoded[col].dtype.name == 'category':\n            le = LabelEncoder()\n            non_null = data_encoded[col].dropna()\n            if len(non_null) > 0:\n                le.fit(non_null)\n                encoders[col] = le\n                mask = data_encoded[col].notna()\n                data_encoded.loc[mask, col] = le.transform(data_encoded.loc[mask, col])\n            data_encoded[col] = pd.to_numeric(data_encoded[col], errors='coerce')\n    \n    imputer = KNNImputer(n_neighbors=n_neighbors)\n    start = time.time()\n    imputed_array = imputer.fit_transform(data_encoded)\n    elapsed = time.time() - start\n    \n    result = pd.DataFrame(imputed_array, columns=data_missing.columns, index=data_missing.index)\n    \n    # Decodificar categóricas\n    for col, le in encoders.items():\n        result[col] = result[col].round().astype(int)\n        result[col] = result[col].clip(0, len(le.classes_) - 1)\n        result[col] = le.inverse_transform(result[col])\n    \n    return result, elapsed\n\n\ndef impute_with_mice(data_missing, max_iter=10):\n    \"\"\"Imputa usando MICE (IterativeImputer) do sklearn.\"\"\"\n    # MICE só funciona com dados numéricos\n    data_encoded = data_missing.copy()\n    encoders = {}\n    \n    for col in data_encoded.columns:\n        if data_encoded[col].dtype == 'object' or data_encoded[col].dtype.name == 'category':\n            le = LabelEncoder()\n            non_null = data_encoded[col].dropna()\n            if len(non_null) > 0:\n                le.fit(non_null)\n                encoders[col] = le\n                mask = data_encoded[col].notna()\n                data_encoded.loc[mask, col] = le.transform(data_encoded.loc[mask, col])\n            data_encoded[col] = pd.to_numeric(data_encoded[col], errors='coerce')\n    \n    imputer = IterativeImputer(max_iter=max_iter, random_state=42)\n    start = time.time()\n    imputed_array = imputer.fit_transform(data_encoded)\n    elapsed = time.time() - start\n    \n    result = pd.DataFrame(imputed_array, columns=data_missing.columns, index=data_missing.index)\n    \n    # Decodificar categóricas\n    for col, le in encoders.items():\n        result[col] = result[col].round().astype(int)\n        result[col] = result[col].clip(0, len(le.classes_) - 1)\n        result[col] = le.inverse_transform(result[col])\n    \n    return result, elapsed\n\n\ndef impute_with_missforest(data_missing, max_iter=10):\n    \"\"\"Imputa usando MissForest.\"\"\"\n    # Tentar diferentes formas de importar o MissForest\n    MissForest = None\n    \n    # Tentativa 1: missforest.MissForest (versão 4.x)\n    try:\n        from missforest import MissForest as MF\n        MissForest = MF\n    except ImportError:\n        pass\n    \n    # Tentativa 2: missforest.missforest.MissForest\n    if MissForest is None:\n        try:\n            from missforest.missforest import MissForest as MF\n            MissForest = MF\n        except ImportError:\n            pass\n    \n    # Tentativa 3: missingpy (pacote alternativo)\n    if MissForest is None:\n        try:\n            from missingpy import MissForest as MF\n            MissForest = MF\n        except ImportError:\n            pass\n    \n    if MissForest is None:\n        print(\"MissForest não disponível. Instalar: pip install missforest ou pip install missingpy\")\n        return data_missing.copy(), np.nan\n    \n    # Preparar dados (MissForest precisa de dados numéricos)\n    data_encoded = data_missing.copy()\n    encoders = {}\n    \n    for col in data_encoded.columns:\n        if data_encoded[col].dtype == 'object' or data_encoded[col].dtype.name == 'category':\n            le = LabelEncoder()\n            non_null = data_encoded[col].dropna()\n            if len(non_null) > 0:\n                le.fit(non_null)\n                encoders[col] = le\n                mask = data_encoded[col].notna()\n                data_encoded.loc[mask, col] = le.transform(data_encoded.loc[mask, col])\n            data_encoded[col] = pd.to_numeric(data_encoded[col], errors='coerce')\n    \n    # Tentar criar instância com diferentes parâmetros\n    try:\n        # Versão 4.x do missforest\n        mf = MissForest(max_iter=max_iter)\n    except TypeError:\n        try:\n            # Versão alternativa sem max_iter\n            mf = MissForest()\n        except Exception as e:\n            print(f\"Erro ao criar MissForest: {e}\")\n            return data_missing.copy(), np.nan\n    \n    start = time.time()\n    try:\n        # fit_transform pode retornar DataFrame ou array\n        imputed_result = mf.fit_transform(data_encoded)\n        \n        if isinstance(imputed_result, pd.DataFrame):\n            result = imputed_result\n        else:\n            result = pd.DataFrame(imputed_result, columns=data_missing.columns, index=data_missing.index)\n    except Exception as e:\n        print(f\"Erro durante imputação MissForest: {e}\")\n        return data_missing.copy(), np.nan\n    \n    elapsed = time.time() - start\n    \n    # Decodificar categóricas\n    for col, le in encoders.items():\n        result[col] = result[col].round().astype(int)\n        result[col] = result[col].clip(0, len(le.classes_) - 1)\n        result[col] = le.inverse_transform(result[col])\n    \n    return result, elapsed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmark-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(data_original, dataset_name, col_types, missing_rates=[0.2, 0.3, 0.4, 0.5],\n",
    "                  patterns=['MCAR', 'MAR', 'MNAR'], methods=['ISCA-k', 'KNN', 'MICE', 'MissForest']):\n",
    "    \"\"\"\n",
    "    Executa benchmark completo para um dataset.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame com resultados\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    pattern_funcs = {\n",
    "        'MCAR': introduce_mcar,\n",
    "        'MAR': introduce_mar,\n",
    "        'MNAR': introduce_mnar\n",
    "    }\n",
    "    \n",
    "    method_funcs = {\n",
    "        'ISCA-k': impute_with_iscak,\n",
    "        'KNN': impute_with_knn,\n",
    "        'MICE': impute_with_mice,\n",
    "        'MissForest': impute_with_missforest\n",
    "    }\n",
    "    \n",
    "    total_runs = len(missing_rates) * len(patterns) * len(methods)\n",
    "    current_run = 0\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        for rate in missing_rates:\n",
    "            # Introduzir missings\n",
    "            data_missing = pattern_funcs[pattern](data_original, rate)\n",
    "            missing_mask = data_missing.isna()\n",
    "            \n",
    "            actual_rate = missing_mask.sum().sum() / data_missing.size\n",
    "            \n",
    "            for method in methods:\n",
    "                current_run += 1\n",
    "                print(f\"  [{current_run}/{total_runs}] {pattern} {int(rate*100)}% - {method}...\", end=\" \")\n",
    "                \n",
    "                try:\n",
    "                    imputed_data, elapsed = method_funcs[method](data_missing)\n",
    "                    \n",
    "                    # Calcular métricas\n",
    "                    metrics = calculate_metrics_per_column(\n",
    "                        data_original, imputed_data, missing_mask, col_types\n",
    "                    )\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Dataset': dataset_name,\n",
    "                        'Pattern': pattern,\n",
    "                        'Missing_Rate': f\"{int(rate*100)}%\",\n",
    "                        'Actual_Rate': f\"{actual_rate*100:.1f}%\",\n",
    "                        'Method': method,\n",
    "                        'R2': metrics['R2'],\n",
    "                        'Pearson': metrics['Pearson'],\n",
    "                        'NRMSE': metrics['NRMSE'],\n",
    "                        'Accuracy': metrics['Accuracy'],\n",
    "                        'Time_s': elapsed\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"OK ({elapsed:.2f}s)\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"ERRO: {e}\")\n",
    "                    results.append({\n",
    "                        'Dataset': dataset_name,\n",
    "                        'Pattern': pattern,\n",
    "                        'Missing_Rate': f\"{int(rate*100)}%\",\n",
    "                        'Actual_Rate': f\"{actual_rate*100:.1f}%\",\n",
    "                        'Method': method,\n",
    "                        'R2': np.nan,\n",
    "                        'Pearson': np.nan,\n",
    "                        'NRMSE': np.nan,\n",
    "                        'Accuracy': np.nan,\n",
    "                        'Time_s': np.nan\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "datasets-header",
   "metadata": {},
   "source": [
    "## Carregar Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IRIS (numérico) ===\n",
    "iris_data = load_iris()\n",
    "data_iris = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
    "col_types_iris = {col: 'numeric' for col in data_iris.columns}\n",
    "print(f\"Iris: {data_iris.shape}\")\n",
    "\n",
    "# === DIABETES (numérico) ===\n",
    "diabetes_data = load_diabetes()\n",
    "data_diabetes = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\n",
    "col_types_diabetes = {col: 'numeric' for col in data_diabetes.columns}\n",
    "print(f\"Diabetes: {data_diabetes.shape}\")\n",
    "\n",
    "# === WINE (assumindo ficheiro local) ===\n",
    "try:\n",
    "    data_wine = pd.read_csv(\"dataset.csv\", sep=\";\")\n",
    "    col_types_wine = {col: 'numeric' for col in data_wine.columns}\n",
    "    print(f\"Wine: {data_wine.shape}\")\n",
    "except:\n",
    "    data_wine = None\n",
    "    print(\"Wine: não encontrado\")\n",
    "\n",
    "# === SONAR (numérico) ===\n",
    "try:\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    sonar_data = fetch_openml('sonar', version=1, parser='auto', as_frame=True)\n",
    "    data_sonar = pd.DataFrame(sonar_data.data).select_dtypes(include=[np.number])\n",
    "    col_types_sonar = {col: 'numeric' for col in data_sonar.columns}\n",
    "    print(f\"Sonar: {data_sonar.shape}\")\n",
    "except:\n",
    "    data_sonar = None\n",
    "    print(\"Sonar: erro ao carregar\")\n",
    "\n",
    "# === TITANIC (misto) ===\n",
    "try:\n",
    "    url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "    data_titanic_raw = pd.read_csv(url)\n",
    "    # Seleccionar colunas úteis e limpar\n",
    "    data_titanic = data_titanic_raw[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']].dropna()\n",
    "    col_types_titanic = {\n",
    "        'Pclass': 'categorical', 'Sex': 'categorical', 'Age': 'numeric',\n",
    "        'SibSp': 'numeric', 'Parch': 'numeric', 'Fare': 'numeric', 'Embarked': 'categorical'\n",
    "    }\n",
    "    print(f\"Titanic: {data_titanic.shape}\")\n",
    "except:\n",
    "    data_titanic = None\n",
    "    print(\"Titanic: erro ao carregar\")\n",
    "\n",
    "# === CREDIT APPROVAL (misto) ===\n",
    "try:\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\"\n",
    "    column_names = ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', \n",
    "                    'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16']\n",
    "    data_credit = pd.read_csv(url, header=None, names=column_names, na_values='?')\n",
    "    data_credit = data_credit.dropna()  # Remover NaN originais para ter ground truth\n",
    "    \n",
    "    # Detectar tipos\n",
    "    col_types_credit = {}\n",
    "    for col in data_credit.columns:\n",
    "        if pd.api.types.is_numeric_dtype(data_credit[col]):\n",
    "            col_types_credit[col] = 'numeric'\n",
    "        else:\n",
    "            col_types_credit[col] = 'categorical'\n",
    "    print(f\"Credit: {data_credit.shape}\")\n",
    "except:\n",
    "    data_credit = None\n",
    "    print(\"Credit: erro ao carregar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-benchmarks-header",
   "metadata": {},
   "source": [
    "## Executar Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-iris",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BENCHMARK: IRIS\")\n",
    "print(\"=\" * 60)\n",
    "results_iris = run_benchmark(data_iris, 'Iris', col_types_iris)\n",
    "display(results_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BENCHMARK: DIABETES\")\n",
    "print(\"=\" * 60)\n",
    "results_diabetes = run_benchmark(data_diabetes, 'Diabetes', col_types_diabetes)\n",
    "display(results_diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-sonar",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_sonar is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BENCHMARK: SONAR\")\n",
    "    print(\"=\" * 60)\n",
    "    results_sonar = run_benchmark(data_sonar, 'Sonar', col_types_sonar)\n",
    "    display(results_sonar)\n",
    "else:\n",
    "    results_sonar = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-titanic",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_titanic is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BENCHMARK: TITANIC (MISTO)\")\n",
    "    print(\"=\" * 60)\n",
    "    results_titanic = run_benchmark(data_titanic, 'Titanic', col_types_titanic)\n",
    "    display(results_titanic)\n",
    "else:\n",
    "    results_titanic = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_credit is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BENCHMARK: CREDIT (MISTO)\")\n",
    "    print(\"=\" * 60)\n",
    "    results_credit = run_benchmark(data_credit, 'Credit', col_types_credit)\n",
    "    display(results_credit)\n",
    "else:\n",
    "    results_credit = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Resultados Consolidados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidate-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar todos os resultados\n",
    "all_results = pd.concat([\n",
    "    results_iris,\n",
    "    results_diabetes,\n",
    "    results_sonar if len(results_sonar) > 0 else pd.DataFrame(),\n",
    "    results_titanic if len(results_titanic) > 0 else pd.DataFrame(),\n",
    "    results_credit if len(results_credit) > 0 else pd.DataFrame()\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"Total de experimentos: {len(all_results)}\")\n",
    "all_results.to_csv('benchmark_results.csv', index=False)\n",
    "print(\"Resultados guardados em 'benchmark_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-by-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo por método\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESUMO POR MÉTODO (MÉDIAS)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary = all_results.groupby('Method').agg({\n",
    "    'R2': 'mean',\n",
    "    'Pearson': 'mean',\n",
    "    'NRMSE': 'mean',\n",
    "    'Accuracy': 'mean',\n",
    "    'Time_s': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-by-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo por padrão de missingness\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESUMO POR PADRÃO DE MISSINGNESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for pattern in ['MCAR', 'MAR', 'MNAR']:\n",
    "    print(f\"\\n{pattern}:\")\n",
    "    pattern_data = all_results[all_results['Pattern'] == pattern]\n",
    "    summary_pattern = pattern_data.groupby('Method').agg({\n",
    "        'R2': 'mean',\n",
    "        'Pearson': 'mean',\n",
    "        'NRMSE': 'mean',\n",
    "        'Accuracy': 'mean',\n",
    "        'Time_s': 'mean'\n",
    "    }).round(4)\n",
    "    display(summary_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-by-rate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo por taxa de missing\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESUMO POR TAXA DE MISSING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for rate in ['20%', '30%', '40%', '50%']:\n",
    "    print(f\"\\n{rate}:\")\n",
    "    rate_data = all_results[all_results['Missing_Rate'] == rate]\n",
    "    summary_rate = rate_data.groupby('Method').agg({\n",
    "        'R2': 'mean',\n",
    "        'Pearson': 'mean',\n",
    "        'NRMSE': 'mean',\n",
    "        'Accuracy': 'mean',\n",
    "        'Time_s': 'mean'\n",
    "    }).round(4)\n",
    "    display(summary_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}